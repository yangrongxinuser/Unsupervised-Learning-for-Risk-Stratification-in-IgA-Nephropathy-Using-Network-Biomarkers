{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEIEamN-cr0U"
      },
      "outputs": [],
      "source": [
        "pip install reval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from scipy import __version__ as scipy_version\n",
        "import joblib\n",
        "from sklearn import __version__ as sklearn_version\n",
        "from umap import __version__ as umap_version\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "U_IP8omFcx45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_graph2vec_embeddings_and_predict_clusters(input_file_path, reference_file_path, output_file_path):\n",
        "    # Load the data from the input Excel file into a Pandas DataFrame\n",
        "    df = pd.read_excel(input_file_path, index_col=0)\n",
        "\n",
        "    # Load the reference samples from the Excel file into a Pandas DataFrame\n",
        "    df_2 = pd.read_excel(reference_file_path, index_col=0)\n",
        "\n",
        "    # Combine both DataFrames to create a single DataFrame containing all samples\n",
        "    reference_df = df_2\n",
        "\n",
        "    # Initialize an empty dictionary to store SSNs for each single sample\n",
        "    ssn_dict = {}\n",
        "\n",
        "    # Iterate through each row as a single test sample\n",
        "    for sample_id, single_sample in df.iterrows():\n",
        "        # Remove the single test sample from the DataFrame for reference samples\n",
        "        reference_samples = reference_df\n",
        "\n",
        "        # Calculate PCC for the reference samples (excluding the first column)\n",
        "        reference_pcc_matrix = reference_samples.corr()\n",
        "\n",
        "        # Construct the perturbed network by adding the single test sample\n",
        "        perturbed_reference_samples = reference_samples._append(single_sample)\n",
        "        perturbed_pcc_matrix = perturbed_reference_samples.corr()\n",
        "\n",
        "        # Calculate the differential network by taking the difference\n",
        "        differential_pcc_matrix = perturbed_pcc_matrix - reference_pcc_matrix\n",
        "\n",
        "        # Define a significance threshold\n",
        "        significance_threshold = 0.05\n",
        "\n",
        "        # Create the SSN for the individual sample based on significant edges\n",
        "        ssn = differential_pcc_matrix[abs(differential_pcc_matrix) > significance_threshold]\n",
        "\n",
        "        # Store the SSN in the dictionary\n",
        "        ssn_dict[sample_id] = ssn\n",
        "\n",
        "    # Define the personalized_networks dictionary to store patient-specific graphs\n",
        "    personalized_networks = {}\n",
        "\n",
        "    # Iterate through each patient's SSN\n",
        "    for patient_id, correlation_matrix in ssn_dict.items():\n",
        "        # Create a NetworkX graph\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # Add nodes (biomarkers)\n",
        "        nodes = list(correlation_matrix.columns)\n",
        "        G.add_nodes_from(nodes)\n",
        "\n",
        "        # Add edges with correlation greater than the significance threshold\n",
        "        for i in range(len(nodes)):\n",
        "            for j in range(i + 1, len(nodes)):\n",
        "                correlation = correlation_matrix.iloc[i, j]\n",
        "                if abs(correlation) > significance_threshold:\n",
        "                    G.add_edge(nodes[i], nodes[j], weight=correlation)\n",
        "\n",
        "        # Store the personalized network\n",
        "        G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
        "        personalized_networks[patient_id] = G\n",
        "\n",
        "    # Initialize an empty dictionary to store Graph2Vec embeddings for each patient\n",
        "    unarrival_graph2vec_embeddings = {}\n",
        "\n",
        "    # Iterate through each patient's graph\n",
        "    for patient_id, patient_graph in personalized_networks.items():\n",
        "        # Convert the NetworkX graph to a format compatible with Karate Club\n",
        "        G = patient_graph\n",
        "\n",
        "        # Find the largest subgraph of the full graph\n",
        "        max_subgraph = max(nx.connected_components(G), key=len)\n",
        "        max_subgraph = G.subgraph(max_subgraph)\n",
        "\n",
        "        # Calculate features of the largest subgraph\n",
        "        num_nodes = len(max_subgraph.nodes)\n",
        "        num_edges = len(max_subgraph.edges)\n",
        "        average_clustering = nx.average_clustering(max_subgraph)\n",
        "        subgraph_diameter = nx.diameter(max_subgraph)\n",
        "\n",
        "        # Create a feature vector for the largest subgraph\n",
        "        subgraph_feature_vector = [num_nodes, num_edges, average_clustering, subgraph_diameter]\n",
        "\n",
        "        # Calculate features of the full graph\n",
        "        num_nodes = len(G.nodes)\n",
        "        num_edges = len(G.edges)\n",
        "        num_connected_components = nx.number_connected_components(G)\n",
        "\n",
        "        # Create a feature vector for the full graph\n",
        "        feature_vector = [num_nodes, num_edges, num_connected_components]\n",
        "        feature_vector.extend(subgraph_feature_vector)\n",
        "\n",
        "        # Calculate node degrees\n",
        "        node_degrees = dict(G.degree())\n",
        "        degree_vector = [node_degrees.get(node, 0) for node in range(24)]\n",
        "\n",
        "        # Add degree vector to the feature vector\n",
        "        feature_vector.extend(degree_vector)\n",
        "\n",
        "        # Store the Graph2Vec embedding for the patient\n",
        "        unarrival_graph2vec_embeddings[patient_id] = feature_vector\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    df_embeddings = pd.DataFrame.from_dict(unarrival_graph2vec_embeddings, orient='index')\n",
        "\n",
        "    # Reset the index to have the subject key as a column\n",
        "    df_embeddings.reset_index(inplace=True)\n",
        "\n",
        "    # Rename the columns\n",
        "    df_embeddings.columns = ['subject_key'] + [f'feature_{i}' for i in range(1, len(df_embeddings.columns))]\n",
        "\n",
        "    # Save the DataFrame to Excel\n",
        "    df_embeddings.to_excel(output_file_path, index=False)\n",
        "\n",
        "    # Now, use the generated embeddings to predict cluster labels\n",
        "    # Load trained models\n",
        "    umap = joblib.load('umap_model.pkl')\n",
        "    scaler = joblib.load('scaler_model.pkl')\n",
        "    imputer = joblib.load('imputer_model.pkl')\n",
        "    kmeans = joblib.load('kmeans_model.pkl')\n",
        "\n",
        "    # Read the input data\n",
        "    X_new = df_embeddings.drop(columns=['subject_key'])\n",
        "\n",
        "    # Impute missing values\n",
        "    X_new_imputed = pd.DataFrame(imputer.transform(X_new), columns=X_new.columns, index=X_new.index)\n",
        "\n",
        "    # Scale the data\n",
        "    X_new_scaled = pd.DataFrame(scaler.transform(X_new_imputed), columns=X_new.columns, index=X_new.index)\n",
        "\n",
        "    # UMAP dimensionality reduction\n",
        "    new_umap_results = pd.DataFrame(index=X_new.index)\n",
        "    for sample_index, sample_row in X_new_scaled.iterrows():\n",
        "        combined_data = pd.concat([X_new_scaled, pd.DataFrame(sample_row).transpose()], axis=0)\n",
        "        umap_result = umap.transform(combined_data)\n",
        "        new_sample_umap_result = pd.DataFrame(umap_result[-1:], columns=[\"UMAP1\", \"UMAP2\"], index=[sample_index])\n",
        "        cluster_labels_new = kmeans.predict(new_sample_umap_result[['UMAP1', 'UMAP2']])\n",
        "        # 定义映射字典\n",
        "        label_mapping = {0: 2, 1: 3, 2: 1, 3: 4}\n",
        "        # 对聚类标签进行映射\n",
        "        mapped_cluster_label = label_mapping.get(cluster_labels_new[0], cluster_labels_new[0])\n",
        "\n",
        "        new_umap_results.loc[sample_index, 'Cluster_Label'] = mapped_cluster_label\n",
        "\n",
        "    # 在生成结果时包含初始文件的Name列信息\n",
        "    df = pd.read_excel(input_file_path)\n",
        "    new_umap_results['Name'] = df[['Name']]\n",
        "\n",
        "    # Save only the Name and Cluster_Label columns to the Excel file\n",
        "    new_umap_results[['Name', 'Cluster_Label']].to_excel(output_file_path, index=False)\n",
        "\n",
        "# Example usage:\n",
        "generate_graph2vec_embeddings_and_predict_clusters(\"/content/your_file.xlsx\", \"/content/reference_samples.xlsx\", \"/content/final_result.xlsx\")\n"
      ],
      "metadata": {
        "id": "3HDSM2QsRMEW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}